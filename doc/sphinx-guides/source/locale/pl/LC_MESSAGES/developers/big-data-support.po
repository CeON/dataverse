# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, The President & Fellows of Harvard College
# This file is distributed under the same license as the Dataverse package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Dataverse 4.11\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-03-13 16:40+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.4.0\n"

#: ../../source/developers/big-data-support.rst:6
msgid "Big Data Support"
msgstr ""

#: ../../source/developers/big-data-support.rst:8
msgid ""
"Big data support is highly experimental. Eventually this content will "
"move to the Installation Guide."
msgstr ""

#: ../../source/developers/big-data-support.rst:13
msgid ""
"Various components need to be installed and configured for big data "
"support."
msgstr ""

#: ../../source/developers/big-data-support.rst:16
msgid "Data Capture Module (DCM)"
msgstr ""

#: ../../source/developers/big-data-support.rst:18
msgid ""
"Data Capture Module (DCM) is an experimental component that allows users "
"to upload large datasets via rsync over ssh."
msgstr ""

#: ../../source/developers/big-data-support.rst:23
msgid "Install a DCM"
msgstr ""

#: ../../source/developers/big-data-support.rst:25
msgid ""
"Installation instructions can be found at https://github.com/sbgrid/data-"
"capture-module/blob/master/doc/installation.md. Note that shared storage "
"(posix or AWS S3) between Dataverse and your DCM is required."
msgstr ""

#: ../../source/developers/big-data-support.rst:29
msgid ""
"Once you have installed a DCM, you will need to configure two database "
"settings on the Dataverse side. These settings are documented in the "
":doc:`/installation/config` section of the Installation Guide:"
msgstr ""

#: ../../source/developers/big-data-support.rst:31
msgid "``:DataCaptureModuleUrl`` should be set to the URL of a DCM you installed."
msgstr ""

#: ../../source/developers/big-data-support.rst:32
msgid "``:UploadMethods`` should include ``dcm/rsync+ssh``."
msgstr ""

#: ../../source/developers/big-data-support.rst:34
msgid ""
"This will allow your Dataverse installation to communicate with your DCM,"
" so that Dataverse can download rsync scripts for your users."
msgstr ""

#: ../../source/developers/big-data-support.rst:37
msgid "Downloading rsync scripts via Dataverse API"
msgstr ""

#: ../../source/developers/big-data-support.rst:39
msgid ""
"The rsync script can be downloaded from Dataverse via API using an "
"authorized API token. In the following curl example substitute "
"``$PERSISTENT_ID`` with a DOI or Handle::"
msgstr ""

#: ../../source/developers/big-data-support.rst:46
msgid "How a DCM reports checksum success or failure to Dataverse"
msgstr ""

#: ../../source/developers/big-data-support.rst:48
msgid ""
"Once the user uploads files to a DCM, that DCM will perform checksum "
"validation and report to Dataverse the results of that validation. The "
"DCM must be configured to pass the API token of a superuser."
msgstr ""

#: ../../source/developers/big-data-support.rst:50
msgid ""
"The JSON that a DCM sends to Dataverse on successful checksum validation "
"looks something like the contents of "
":download:`checksumValidationSuccess.json "
"<../_static/installation/files/root/big-data-"
"support/checksumValidationSuccess.json>`:"
msgstr ""

#: ../../source/developers/big-data-support.rst:55
msgid "Where:"
msgstr ""

#: ../../source/developers/big-data-support.rst:57
msgid ""
"``status`` - The valid strings to send are ``validation passed`` and "
"``validation failed``."
msgstr ""

#: ../../source/developers/big-data-support.rst:58
msgid ""
"``uploadFolder`` - This is the directory on disk where Dataverse should "
"attempt to find the files that a DCM has moved into place. There should "
"always be a ``files.sha`` file and a least one data file. ``files.sha`` "
"is a manifest of all the data files and their checksums. The "
"``uploadFolder`` directory is inside the directory where data is stored "
"for the dataset and may have the same name as the \"identifier\" of the "
"persistent id (DOI or Handle). For example, you would send "
"``\"uploadFolder\": \"DNXV2H\"`` in the JSON file when the absolute path "
"to this directory is "
"``/usr/local/glassfish4/glassfish/domains/domain1/files/10.5072/FK2/DNXV2H/DNXV2H``."
msgstr ""

#: ../../source/developers/big-data-support.rst:59
msgid ""
"``totalSize`` - Dataverse will use this value to represent the total size"
" in bytes of all the files in the \"package\" that's created. If 360 data"
" files and one ``files.sha`` manifest file are in the ``uploadFolder``, "
"this value is the sum of the 360 data files."
msgstr ""

#: ../../source/developers/big-data-support.rst:62
msgid "Here's the syntax for sending the JSON::"
msgstr ""

#: ../../source/developers/big-data-support.rst:68
msgid "Steps to set up a DCM mock for Development"
msgstr ""

#: ../../source/developers/big-data-support.rst:70
msgid ""
"See instructions at https://github.com/sbgrid/data-capture-"
"module/blob/master/doc/mock.md"
msgstr ""

#: ../../source/developers/big-data-support.rst:73
msgid ""
"Add Dataverse settings to use mock (same settings as when using non-mock "
"DCM noted in :ref:`install_dcm`)::"
msgstr ""

#: ../../source/developers/big-data-support.rst:78
msgid ""
"At this point you should be able to download a placeholder rsync script. "
"Dataverse is then waiting for news from the DCM about if checksum "
"validation has succeeded or not. First, you have to put files in place, "
"which is usually the job of the DCM. You should substitute \"X1METO\" for"
" the \"identifier\" of the dataset you create. You must also use the "
"proper path for where you store files in your dev environment::"
msgstr ""

#: ../../source/developers/big-data-support.rst:88
msgid ""
"Now the files are in place and you need to send JSON to Dataverse with a "
"success or failure message as described in "
":ref:`dcm_success_failure_report`. Make a copy of ``doc/sphinx-"
"guides/source/_static/installation/files/root/big-data-"
"support/checksumValidationSuccess.json`` and put the identifier in place "
"such as \"X1METO\" under \"uploadFolder\"). Then use curl as described in"
" :ref:`dcm_success_failure_report` to send the JSON."
msgstr ""

#: ../../source/developers/big-data-support.rst:91
msgid "Troubleshooting"
msgstr ""

#: ../../source/developers/big-data-support.rst:93
msgid ""
"The following low level command should only be used when troubleshooting "
"the \"import\" code a DCM uses but is documented here for completeness::"
msgstr ""

#: ../../source/developers/big-data-support.rst:98
msgid "Steps to set up a DCM via Docker for Development"
msgstr ""

#: ../../source/developers/big-data-support.rst:100
msgid ""
"If you need a fully operating DCM client for development purposes, these "
"steps will guide you to setting one up. This includes steps to set up the"
" DCM on S3 variant."
msgstr ""

#: ../../source/developers/big-data-support.rst:103
msgid "Docker Image Set-up"
msgstr ""

#: ../../source/developers/big-data-support.rst:105
msgid ""
"See https://github.com/CeON/dataverse/blob/develop/conf/docker-"
"dcm/readme.txt"
msgstr ""

#: ../../source/developers/big-data-support.rst:107
msgid "Install docker if you do not have it"
msgstr ""

#: ../../source/developers/big-data-support.rst:110
msgid "Optional steps for setting up the S3 Docker DCM Variant"
msgstr ""

#: ../../source/developers/big-data-support.rst:112
msgid ""
"Before: the default bucket for DCM to hold files in S3 is named test-dcm."
" It is coded into `post_upload_s3.bash` (line 30). Change to a different "
"bucket if needed."
msgstr ""

#: ../../source/developers/big-data-support.rst:114
msgid "Add AWS credentials to ``~/.aws/credentials`` (on dcmsrv)::"
msgstr ""

#: ../../source/developers/big-data-support.rst:120
msgid "Dataverse configuration (on dvsrv):"
msgstr ""

#: ../../source/developers/big-data-support.rst:122
msgid "Set S3 as the storage driver::"
msgstr ""

#: ../../source/developers/big-data-support.rst:128
msgid "Add AWS credentials to ``~/.aws/credentials``::"
msgstr ""

#: ../../source/developers/big-data-support.rst:134
msgid ""
"Also: set region in ``~/.aws/config`` to create a region file. Add these "
"contents::"
msgstr ""

#: ../../source/developers/big-data-support.rst:139
msgid "Add the S3 bucket names to Dataverse"
msgstr ""

#: ../../source/developers/big-data-support.rst:141
msgid "S3 bucket for Dataverse::"
msgstr ""

#: ../../source/developers/big-data-support.rst:145
msgid "S3 bucket for DCM (as Dataverse needs to do the copy over)::"
msgstr ""

#: ../../source/developers/big-data-support.rst:149
msgid ""
"Set download method to be HTTP, as DCM downloads through S3 are over this"
" protocol::"
msgstr ""

#: ../../source/developers/big-data-support.rst:154
msgid "Using the DCM Docker Containers"
msgstr ""

#: ../../source/developers/big-data-support.rst:156
msgid ""
"For using these commands, you will need to connect to the shell prompt "
"inside various containers (e.g. ``docker exec -it dvsrv /bin/bash``)"
msgstr ""

#: ../../source/developers/big-data-support.rst:158
msgid "Create a dataset and download rsync upload script"
msgstr ""

#: ../../source/developers/big-data-support.rst:160
msgid "connect to client container: ``docker exec -it dcm_client bash``"
msgstr ""

#: ../../source/developers/big-data-support.rst:161
msgid ""
"create dataset: ``cd /mnt ; ./create.bash`` ; this will echo the database"
" ID to stdout"
msgstr ""

#: ../../source/developers/big-data-support.rst:162
msgid ""
"download transfer script: ``./get_transfer.bash "
"$database_id_from_create_script``"
msgstr ""

#: ../../source/developers/big-data-support.rst:163
msgid ""
"execute the transfer script: ``bash ./upload-${database_id_from-"
"create_script}.bash`` , and follow instructions from script."
msgstr ""

#: ../../source/developers/big-data-support.rst:165
msgid "Run script"
msgstr ""

#: ../../source/developers/big-data-support.rst:167
msgid ""
"e.g. ``bash ./upload-3.bash`` (``3`` being the database id from earlier "
"commands in this example)."
msgstr ""

#: ../../source/developers/big-data-support.rst:169
msgid "Manually run post upload script on dcmsrv"
msgstr ""

#: ../../source/developers/big-data-support.rst:171
msgid ""
"for posix implementation: ``docker exec -it dcmsrv "
"/opt/dcm/scn/post_upload.bash``"
msgstr ""

#: ../../source/developers/big-data-support.rst:172
msgid ""
"for S3 implementation: ``docker exec -it dcmsrv "
"/opt/dcm/scn/post_upload_s3.bash``"
msgstr ""

#: ../../source/developers/big-data-support.rst:175
msgid "Additional DCM docker development tips"
msgstr ""

#: ../../source/developers/big-data-support.rst:177
msgid ""
"You can completely blow away all the docker images with these commands "
"(including non DCM ones!) - ``docker-compose -f docmer-compose.yml down "
"-v``"
msgstr ""

#: ../../source/developers/big-data-support.rst:180
msgid "There are a few logs to tail"
msgstr ""

#: ../../source/developers/big-data-support.rst:182
msgid ""
"dvsrv : ``tail -n 2000 -f "
"/opt/glassfish4/glassfish/domains/domain1/logs/server.log``"
msgstr ""

#: ../../source/developers/big-data-support.rst:183
msgid "dcmsrv : ``tail -n 2000 -f /var/log/lighttpd/breakage.log``"
msgstr ""

#: ../../source/developers/big-data-support.rst:184
msgid "dcmsrv : ``tail -n 2000 -f /var/log/lighttpd/access.log``"
msgstr ""

#: ../../source/developers/big-data-support.rst:186
msgid ""
"You may have to restart the glassfish domain occasionally to deal with "
"memory filling up. If deployment is getting reallllllly slow, its a good "
"time."
msgstr ""

#: ../../source/developers/big-data-support.rst:189
msgid "Repository Storage Abstraction Layer (RSAL)"
msgstr ""

#: ../../source/developers/big-data-support.rst:192
msgid "Configuring the RSAL Mock"
msgstr ""

#: ../../source/developers/big-data-support.rst:194
msgid ""
"Info for configuring the RSAL Mock: "
"https://github.com/sbgrid/rsal/tree/master/mocks"
msgstr ""

#: ../../source/developers/big-data-support.rst:196
msgid ""
"Also, to configure Dataverse to use the new workflow you must do the "
"following (see also the :doc:`workflows` section):"
msgstr ""

#: ../../source/developers/big-data-support.rst:198
msgid "Configure the RSAL URL::"
msgstr ""

#: ../../source/developers/big-data-support.rst:202
msgid ""
"Update workflow json with correct URL information. Edit internal-httpSR-"
"workflow.json and replace url and rollbackUrl to be the url of your RSAL "
"mock."
msgstr ""

#: ../../source/developers/big-data-support.rst:204
msgid "Create the workflow::"
msgstr ""

#: ../../source/developers/big-data-support.rst:208
msgid "List available workflows::"
msgstr ""

#: ../../source/developers/big-data-support.rst:212
msgid ""
"Set the workflow (id) as the default workflow for the appropriate "
"trigger::"
msgstr ""

#: ../../source/developers/big-data-support.rst:216
msgid "Check that the trigger has the appropriate default workflow set::"
msgstr ""

#: ../../source/developers/big-data-support.rst:220
msgid "Add RSAL to whitelist"
msgstr ""

#: ../../source/developers/big-data-support.rst:222
msgid "When finished testing, unset the workflow::"
msgstr ""

#: ../../source/developers/big-data-support.rst:227
msgid "Configuring download via rsync"
msgstr ""

#: ../../source/developers/big-data-support.rst:229
msgid "In order to see the rsync URLs, you must run this command::"
msgstr ""

#: ../../source/developers/big-data-support.rst:235
msgid "To specify replication sites that appear in rsync URLs:"
msgstr ""

#: ../../source/developers/big-data-support.rst:237
msgid ""
"Download :download:`add-storage-site.json "
"<../../../../scripts/api/data/storageSites/add-storage-site.json>` and "
"adjust it to meet your needs. The file should look something like this:"
msgstr ""

#: ../../source/developers/big-data-support.rst:241
msgid "Then add the storage site using curl::"
msgstr ""

#: ../../source/developers/big-data-support.rst:245
msgid ""
"You make a storage site the primary site by passing \"true\". Pass "
"\"false\" to make it not the primary site. (id \"1\" in the example)::"
msgstr ""

#: ../../source/developers/big-data-support.rst:249
msgid "You can delete a storage site like this (id \"1\" in the example)::"
msgstr ""

#: ../../source/developers/big-data-support.rst:253
msgid "You can view a single storage site like this: (id \"1\" in the example)::"
msgstr ""

#: ../../source/developers/big-data-support.rst:257
msgid "You can view all storage site like this::"
msgstr ""

#: ../../source/developers/big-data-support.rst:261
msgid ""
"In the GUI, this is called \"Local Access\". It's where you can compute "
"on files on your cluster::"
msgstr ""

#: ../../source/developers/big-data-support.rst:266
msgid "Previous: :doc:`selinux` | Next: :doc:`workflows`"
msgstr ""

